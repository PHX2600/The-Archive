## Vulnerability disclosure politics <rant>
Posted by **XlogicX** on Fri January 20th, 2012 11:29:49 PM

I want to agree with the white hat ethic of disclosing a vulnerability to a vendor before the public with ample time to fix it...but with how vendors seem to treat disclosures, I kind of understand why some hackers and security researchers lose their patience.

This is the new one in the news:
<!-- m --><a class="postlink" href="http://www.wired.com/threatlevel/2012/01/scada-exploits/">http://www.wired.com/threatlevel/2012/0 ... -exploits/</a><!-- m -->

Some researchers released a bit of metasploit modules for SCADA systems (industrial controls). The vulnerable systems where:
General Electric D20ME
Koyo/Direct LOGIC H4-ES
Rockwell Automation/Allen-Bradley ControlLogix
Rockwell Automation/Allen-Bradley MicroLogix
Schneider Electric Modicon Quantum
Schweitzer SEL-2032 (a communication module for relays)

Anyway, the researchers discussed this stuff at S4 and a DHS official went a little nuts I guess, saying they should have disclosed this stuff to the vendors first. Then one of the researchers snapped back saying that these vulnerabilities have been commonly known for years. In fact, he made the point that all a vendor does with a disclosure is downplays it to the public and still ignores it.

A better case-in-point is FireSheep. We had to deal with large websites (facebook) not using universal https, not taking the security truly seriously because of how impractical the hacking was (debatable). Then firesheep comes out to make it even easier, then the websites start offering https.

The takeaway is that vendors don't give a shit until it actually hurts them. And how dare the DHS heckle the researchers, our own government has been publicly mentioning how in-need they are of improved computer security of their own. Our government has troubles keeping fbi.gov and justice.gov online from a DoS, they should stay out of the judgemental conversation.

Although I ethically disagree with the recent credit-card and password dumps on pastebin, it doesn't mean that I don't understand why it's done. It's not to screw the customers (although it effectively does), it's because the companies don't give a shit about how vulnerable the customer data is until it makes them actually hurt (and a private disclosure doesn't make them hurt)

...Sony ::cough::

</rant>

--------------------------------------------------------------------------------

Posted by **XlogicX** on Fri January 20th, 2012 11:45:40 PM

Zappos...now getting sued for security negligence

<!-- m --><a class="postlink" href="http://www.bankinfosecurity.com/articles.php?art_id=4422">http://www.bankinfosecurity.com/article ... rt_id=4422</a><!-- m -->

--------------------------------------------------------------------------------

Posted by **AltF4** on Sat January 21st, 2012 08:17:04 AM

I think it has everything to do with the upstream authors in question. (I prefer not to refer to software as something that can be vended, like a Snickers bar) And sometimes the severity of the bug. If I found a serious vulnerability in Firefox or Chrome, then I would have no qualms with giving Mozilla and Google a lead time in order to fix the bug before disclosing it. Because Mozilla and Google have proven over time that they genuinely care about security. Punishing them by having a 0-day out against their browsers won't give them any more impetus to be more secure since they're already doing their best.

But if I were to find a vulnerability in IE, then I can't say the same about Microsoft. A 0-day might be just the thing to get them to care a little more about security.

Though certain bugs like the Kaminsky DNS bug a few years back are just too big to play politics with. That needed to be fixed or the Internet as a whole would start getting hosed.

--------------------------------------------------------------------------------

Posted by **XlogicX** on Sat January 21st, 2012 08:26:41 AM

Totally agrees with Mozilla and Google, not only are they very diligent, but they actually pay people to find and disclose bugs. I guess its got to be kind of case-by-case then. Mozilla > Oracle.

Thanks for keeping me in check AltF4

--------------------------------------------------------------------------------

Posted by **Valveritas** on Sat January 21st, 2012 09:28:53 AM

Google cares about security, since they have a close working relationship with the National Security Agency.  At least, the Electronic Privacy Information Center filed an [url=http://epic.org/privacy/nsa/foia/EPIC-v-NSA-OB-FINAL.pdf:2jkbnikt]opening brief> on the subject.

Not to mention, ex-CIA agent Robert David Steele has claimed sources told him that CIA seed money helped get the company off the ground.

--------------------------------------------------------------------------------

Posted by **ArchAngel** on Mon January 23rd, 2012 10:28:08 AM

Trimmed a bit for brevity, but wanted to address this:
[quote="AltF4":3gaduhxw]Because Mozilla and Google have proven over time that they genuinely care about security. Punishing them by having a 0-day out against their browsers won't give them any more impetus to be more secure since they're already doing their best.

But if I were to find a vulnerability in IE, then I can't say the same about Microsoft. A 0-day might be just the thing to get them to care a little more about security.
[/quote:3gaduhxw]

So it seems like if you use perception of how much a corporation or organization cares as criteria for whether or not you should disclose vulnerabilities to them -- you would always choose not to disclose vulnerabilities.

Security-conscious vendors ('upstream authors', &amp;c;) should find the bug you're going to disclose to them on their own ... because they're 'already doing their best'. Early disclosure doensn't necessarily help the vendor -- firstly, it may be something they're already aware of and haven't figured out a good way to patch; destruction is easier than creation. Secondly, your choice to disclose to them because they care so much is really saying "They're trying their best, but they're _never_ going to find this unless I say something" -- if that's really true, then they're not really doing their best, and they'll never improve on the process. More succinctly: If they care about security, they are either experts and don't need your help, or they are amateurs and need to be taught to fish rather than handed fish. Or go work for them directly. 

If the vendor you've found an exploit for does not care about security -- they're not going to find the bug on their own and neither will they see your disclosure in a positive light; there's not really any point in letting them know about the problem.
 
ArchAngel's private criteria is: Do I have a patch for the bug, or a work-around on how to bypass the vulnerability? (e.g.: enable HTTPS on your WFEs?) Then disclose _both_. 

No patch or work-around? Then telling them early will just make them panic -- like being drowned by someone you're trying to save. 

And what if someone else figures out the vulnerability before the vendor has a patch ready and wreaks havoc? Honestly, you could take that same idea and turn it the other way -- what if they're so spun up trying to patch the exploit I've dropped in their lap that they don't correct a different attack vector?

Just my ¥200 ... ^_-.

--------------------------------------------------------------------------------

Posted by **PHLAK** on Mon January 23rd, 2012 11:09:39 AM

[quote="ArchAngel":1eziozz8]Security-conscious vendors ('upstream authors', &amp;c;) should find the bug you're going to disclose to them on their own ... because they're 'already doing their best'. Early disclosure doensn't necessarily help the vendor -- firstly, it may be something they're already aware of and haven't figured out a good way to patch; destruction is easier than creation. Secondly, your choice to disclose to them because they care so much is really saying "They're trying their best, but they're _never_ going to find this unless I say something" -- if that's really true, then they're not really doing their best, and they'll never improve on the process. More succinctly: If they care about security, they are either experts and don't need your help, or they are amateurs and need to be taught to fish rather than handed fish. Or go work for them directly.[/quote:1eziozz8]

I don't agree with this and here's an analogy to help explain why:

Imagine you're in a marathon pitted against an olympic marthon runner.  Assuming you're not also an olymipic runner, your best will not be good enough to beat the olympic runner.  The same goes for these vendors (Mozilla, Google, etc.).  Sure they try to hire the best programmers and security people, but someone who's better trained or just has a better understanding of certain areas will be able to find a vulnerability that, even given months, the ventor's developers wont find.

Also, as a developer working on any one project for a long period of time you tend to get tunnel vision.  You built the product, you know how it works from the inside out, so you might miss something that an above average user might see because of their perspective.  It's always good to hear these users thoughts or findings.

--------------------------------------------------------------------------------

Posted by **Junker** on Wed January 25th, 2012 01:51:33 AM

Not to mention, that you can't even begin to imagine every possible way in which an exploit may attack your program, When you design the software you know how it is _supposed_ to work, and you can envision ways in which people may try to attack it, however the best exploits usually come from things you never imagined. 

Years ago I wrote an online video poker just for fun. I spent a lot of time trying every conceivable thing I could think of to see how to "break" it. The credits were just given out daily, and you could clear cookies and just get more credits if you wanted that way. Like I said, it was just for fun. A lot of people used it, I didn't have any problems with it for a long time, then one day I see the board that tracks the top winners jump from $50,000 - $100,000 averages to 50 to 100 million for a couple players, within a week it had spread to fill the entire leader board which tracked the top 50 players. The odds of someone actually winning that much are astronomically low. Turns out that someone figured out if you win a hand, you can move your cookie out of the directory, move in a copy of the initial cookie that you start with, switch the credit amounts mid-game, so if you are playing the nickel video poker, hit a good hand, you can change the denomination to $100 and get paid as if you had played the $100 video poker, then swap your cookies back. You keep playing cheap, and winning big with that cookie accumulating all your winnings. They told a couple people through pm, who told a few more, who told a few more, and it went around that it could be done. 

It was silly and obvious once it had been pointed out because of the way I'd set up the cookies, but it was there. My first reactions was "why would someone even try that" my second was "why would this many people go through the trouble, just to get their name on the scoreboard". People actually spent hours swapping cookies back and forth just to accumulate large winnings and get their name on the scoreboard, many of them for hours a day, every day for a couple weeks before I fixed it. I knew of some shortcomings of storing their amounts in a cookie, but encrypted it with my own formula that changed on each hand based on several pieces of data and figured i was fairly safe from somebody figuring out how it was done. I never imagined that people, and not just a few of them, but a lot of them, would go through the trouble of moving cookies while playing and maintaining two sets to cheat on a free game.

--------------------------------------------------------------------------------

Posted by **XlogicX** on Wed January 25th, 2012 02:41:48 AM

I gotta say that I'm somewhere in the middle of Phlak and ArchAngel. I would take it case by case with my criteria being the track record of the company. I see what ArchAngel is saying about how it really is the job of the company to keep their code secure, on the other hand, I do think that some things can slip through the cracks. What is most relevant to me is what the company does with a disclosure when it comes their way. If 10 disclosures are made to Google (that google didn't find), but they promptly patch each one and even pay the people that found them, I'm fine with that (even though Google should have found them on their own...we could argue that things still slip through the cracks).

This is to compare with a company that receives a disclosure, actually already knew about it, goes on a PR rampage talking about how "irrelevant" or "theoretical" the vulnerability is, then gets compromised and leaks all of its customer data. That is the scenario I am not fine with. I hear Oracle is pretty bad with this.

Oh, and I forget the details on the WEP historyz, but here's an excerpt from a Security+ prep book:
[quote:2urpxwzv]A paper was submitted to the Internet community that discussed a theoretical weakness
in the algorithm used as the basis for the Wired Equivalent Privacy (WEP) security system.
WEP supporters publicly discounted the weakness to the computer community: They indi-
cated that the vulnerability was theoretical and couldn’t happen in the real world. Within
seven days of their brash statements, they received over a dozen different examples of how
to break the WEP system.
[/quote:2urpxwzv]

WEP sucks, blah blah blah. Regardless, it's the arrogance of big companies and currently SCADA systems owners that are bugging me. I put them in the category of just pandering their PR lines and not taking anything seriously. It's going to be a lulzy next couple of years, that's for damn sure.

--------------------------------------------------------------------------------

Posted by **XlogicX** on Thu May 3rd, 2012 04:16:13 AM

Some more "forever-day" vulns where there was a proper attempt at non-public disclosure, and the hacker was ignored.

Those RuggedCom switches (owned by Siemens now) had some dev backdoors in them that were not removed by the devs. RuggedCom was told about this vuln and ignored the suggestion to fix it. The white-hat even notified Department of Homeland Security’s Industrial Control System Cyber Emergency Response Team and RuggedCom still sat on it.

Well, it went public around sometime last week. The only fix (about 2 weeks around the corner) will have to be a firmware upgrade. This will be costly (in downtime and risk) for many larger corporations.

Default backdoor login: factory
The password is based on the MAC

<!-- m --><a class="postlink" href="http://www.wired.com/threatlevel/2012/04/ruggedcom-to-fix-vuln/">http://www.wired.com/threatlevel/2012/0 ... -fix-vuln/</a><!-- m -->
